version: '3.5'

services:
# https://hub.docker.com/r/tchiotludo/akhq
# https://github.com/tchiotludo/akhq
    akhq:
        container_name: akhq
        dns: 10.3.1.1
        dns_search:
            - ubuntu.home
            - home
        environment:
            AKHQ_CONFIGURATION: |
                akhq:
                    connections:
                        docker-kafka-server:
                            properties:
                                bootstrap.servers: "s3.ubuntu.home:9092,s4.ubuntu.home:9092,s8.home.ubuntu:9092"
            JVM_OPTS_FILE: /app/yaml/jvm.options
            JVM_OPTS: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.port=7203 -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.rmi.port=7203 -Dcom.sun.management.jmxremote.ssl=false -Djava.awt.headless=true -Duser.timezone=UTC -Dmicronaut.config.files=/app/yaml/application.yml"
        hostname: akhq.s4.home
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}thirdparty/akhq:${AKHQ_VERSION:-0.17.0}
        logging:
            driver: json-file
            options:
                max-file: "3"
                max-size: "10m" 
        networks:
            - kafka-net
        ports:
            - "8998:8080"
        volumes:
            - ./akhq/log:/var/log
            - ./akhq/yaml:/app/yaml


    broker:
        command: confluent broker
        container_name: broker
        depends_on:
            - zookeeper
        dns: 10.3.1.1
        dns_search:
            - ubuntu.home
            - home
        environment:
            DEBUG_TRACE: ${DEBUG_TRACE:-0}
            JAVA_HOME: /usr/lib/jvm/java-11-openjdk
            KAFKA_ADVERTISED_HOST_NAME: ${HOST_IP:?}
            KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://${HOST_IP:?}:9092
#            KAFKA_ADVERTISED_LISTENERS: SASL_PLAINTEXT://${HOST_IP:?}:9092
            KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
            KAFKA_BOOTSTRAP_SERVERS: $KAFKA_BOOTSTRAP_SERVERS
            KAFKA_BROKER_ID: ${KAFKA_ID:?}
            KAFKA_COMPRESSION_TYPE: gzip
            KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE: 'true'
            KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
            KAFKA_HEAP_OPTS: "-Xmx6G -Xms6G"
            KAFKA_JMX_OPTS: "-Dcom.sun.management.jmxremote=true -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=${HOST_IP} -Djava.net.preferIPv4Stack=true -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.rmi.port=${JMX_PORT} -Dcom.sun.management.jmxremote.port=${JMX_PORT} -Dzookeeper.sasl.client=ZkClient"
            KAFKA_JMX_PORT: ${JMX_PORT:?}
            KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
#            KAFKA_LISTENERS: SASL_PLAINTEXT://0.0.0.0:9092
            KAFKA_NUM_PARTITIONS: 1
            KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
            KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
            KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
            KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
            KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
            KAFKA_ZOOKEEPER_CONNECT: ${ZOOKEEPER_HOSTS:?}
            ZOOKEEPER_TIMEOUT: 6000
        hostname: broker.s4.home
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}${CONTAINER_OS:-alpine}/confluent/${CONFLUENT_VERSION:-6.1.1}:${CONTAINER_TAG:-4e99ef50a430ef5efd5878e55d49e1ae499cee9d3c214decd055ffa78b6b13a3}
        logging:
            driver: json-file
            options:
                max-file: "3"
                max-size: "10m"
        links:
           - zookeeper
        networks:
            - kafka-net
        ports:
            - "${JMX_PORT}:${JMX_PORT}"
            - "9092:9092"
        restart: unless-stopped
        volumes:
            - ./broker/data:/usr/local/confluent/data
            - ./broker/logs:/usr/local/confluent/logs
            - ./broker/log:/var/log


# https://docs.kafka-eagle.org/2.env-and-install/2.installing
    kafkaeagle:
        container_name: kafka-eagle
        dns: 10.3.1.1
        dns_search:
            - ubuntu.home
            - home
        environment:
            KE_HOME: /opt/kafka-eagle
            KAFKA_ZOOKEEPER_HOSTS: ${ZOOKEEPER_HOSTS:?}
        hostname: kafkaeagle.s4.home
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}${CONTAINER_OS:-alpine}/kafka-eagle/${KAFKA_EAGLE_VERSION:-2.0.4}:${CONTAINER_TAG:-7bd0eeeafd874996c630360e45082990f50b4f61ca5919b47dab2bb108487bcb}
        networks:
            - kafka-net
        ports:
            - "8048:8048"
            - "8996:8080"
        restart: unless-stopped
        volumes:
            - ./kafkaeagle/conf:/opt/kafka-eagle/conf
            - ./kafkaeagle/log:/opt/kafka-eagle/logs


    kafkamgr:
        container_name: kafkamgr
        environment:
            APPLICATION_SECRET: letmein
#            BASE_ZK_PATH:
            DEBUG_TRACE: ${DEBUG_TRACE:-0}
            CONSUMER_PROPERTIES_FILE: /usr/local/cmak/conf/consumer.properties
            KAFKA_MANAGER_PASSWORD_FILE: /run/secrets/kafka_manager.pwd
            KAFKA_MANAGER_USERNAME: admin
            KAFKA_MANAGER_AUTH_ENABLED: 'false'
            KAFKA_MANAGER_LOGLEVEL: INFO
#            KM_HTTP_CONTEXT: /
            KM_HTTP_CONTEXT: /kafkamgr
            ZK_HOSTS: zookeeper:2181
        hostname: kafkamgr
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}${CONTAINER_OS:-alpine}/kafka-manager/${KM_VERSION:-3.0.0.5}:${CONTAINER_TAG:-70d768239067f21f764f89eb7f6aaa9c8bab3bf85d2e5570f753f36871bd994e}
        logging:                              # limit size of logs @runtime so that we do not run out of space 
            driver: json-file
            options:
                max-file: "3"
                max-size: "10m"
        networks:
            - kafka-net
        ports:                                #ports exposed 
            - 9000:9000
        restart: unless-stopped               # only use when building a 'service container'
        secrets:                              # references to 'secrets' section below 
            - kafka.secret
            - kafka_manager.pwd
        volumes:
            - ./cmak:/usr/local/cmak/home             # do not save aplication logs inside container 
            - ./cmak/log:/var/log
     

#    metricbeat:
#        container_name: metricbeat
#        dns: 10.3.1.1
#        dns_search:
#            - ubuntu.home
#            - home
#        environment:
#            OUTPUT_ELASTICSEARCH_HOSTS: "s6.ubuntu.home:9200"
#            SETUP_KIBANA_HOST: "s6.ubuntu.home:5601" 
#        extra_hosts:
#            - "elasticsearch:10.3.1.15" 
#            - "kibana:10.3.1.15" 
#        hostname: metricbeat
#        image: ${DOCKER_REGISTRY:-ubuntu-s2.home:5000/}thirdparty/metricbeat:${METRICBEAT_VERSION:-7.9.3}
#        labels:
#            co.elastic.metrics/hosts: '$${data.host}:$${data.port}'
#            co.elastic.metrics/metricsets: status
#            co.elastic.metrics/module: metricbeat
#            co.elastic.metrics/period: 10s
#        logging:
#            driver: json-file
#            options:
#                max-file: "3"
#                max-size: "10m"
#        networks:
#            - kafka-net
#        restart: unless-stopped
#        user: root
#        volumes:
##            - ./metricbeat/metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
##            - ./metricbeat/metricbeat.yml:/etc/metricbeat/metricbeat.yml:ro
#            - ./metricbeat/modules.d:/usr/share/metricbeat/modules.d:ro
#            - ./metricbeat/log:/var/log
#            - /var/run/docker.sock:/var/run/docker.sock
#            - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
#            - /proc:/hostfs/proc:ro
#            - /:/hostfs:ro
#
#
    zookeeper:
        command: confluent zookeeper
        container_name: zookeeper
        dns: 10.3.1.1
        dns_search:
            - ubuntu.home
            - home
        environment:
            DEBUG_TRACE: ${DEBUG_TRACE:-0}
            JAVA_HOME: /usr/lib/jvm/java-11-openjdk
            ZOOKEEPER_SERVER_ID: ${KAFKA_ID:?}
            ZOOKEEPER_CLIENTPORT: 2181
            ZOOKEEPER_TICKTIME: 2000
            ZOOKEEPER_INITLIMIT: 10
            ZOOKEEPER_SYNCLIMIT: 5
            ZOOKEEPER_SERVER.1: s3.ubuntu.home:2888:3888
            ZOOKEEPER_SERVER.2: 0.0.0.0:2888:3888
            ZOOKEEPER_SERVER.3: s8.ubuntu.home:2888:3888
        hostname: zookeeper.s4.home
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}${CONTAINER_OS:-alpine}/confluent/${CONFLUENT_VERSION:-6.1.1}:${CONTAINER_TAG:-4e99ef50a430ef5efd5878e55d49e1ae499cee9d3c214decd055ffa78b6b13a3}
        logging:
            driver: json-file
            options:
                max-file: "3"
                max-size: "10m"
        networks:
            - kafka-net
        ports:
            - "2181:2181"
            - "2888:2888"
            - "3888:3888"
        restart: unless-stopped
        volumes:
            - ./zookeeper/data:/usr/local/confluent/data
            - ./zookeeper/logs:/usr/local/confluent/logs 
            - ./zookeeper/log:/var/log


#
# docker.io/elkozmon/zoonavigator
#
    zoonavigator:
        container_name: zoonavigator
        dns: 10.3.1.1
        dns_search:
            - ubuntu.home
            - home
        environment:
            DEBUG_TRACE: ${DEBUG_TRACE:-0}
            HTTP_PORT: 9000
        hostname: zoonavigator.s4.home
        image: ${DOCKER_REGISTRY:-s2.ubuntu.home:5000/}thirdparty/zoonavigator:${ZOONAVIGATOR_VERSION:-1.1.0}
        logging:
            driver: json-file
            options:
                max-file: "3"
                max-size: "10m"
#        network_mode: "host"
        networks:
            - kafka-net
        ports:
            - "8999:9000"
        restart: unless-stopped
        volumes:
            - ./zoonavigator/log:/var/log

 
networks:
   kafka-net:
       name: kafka-container-net


secrets:
   kafka.secret:
     file: .secrets/secrets/kafkamgr.secret
   kafka_manager.pwd:
     file: .secrets/secrets/bobb.pwd
